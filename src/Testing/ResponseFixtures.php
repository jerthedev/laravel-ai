<?php

namespace JTD\LaravelAI\Testing;

/**
 * Response Fixtures
 *
 * Library of realistic AI responses for different providers and scenarios.
 * Used by MockProvider and testing utilities to provide consistent test data.
 */
class ResponseFixtures
{
    /**
     * Get all available fixtures.
     */
    public static function all(): array
    {
        return [
            'openai' => self::openaiFixtures(),
            'xai' => self::xaiFixtures(),
            'gemini' => self::geminiFixtures(),
            'ollama' => self::ollamaFixtures(),
            'generic' => self::genericFixtures(),
            'errors' => self::errorFixtures(),
        ];
    }

    /**
     * Get fixtures for a specific provider.
     */
    public static function forProvider(string $provider): array
    {
        $fixtures = self::all();

        return $fixtures[$provider] ?? $fixtures['generic'];
    }

    /**
     * Get a specific fixture by provider and key.
     */
    public static function get(string $provider, string $key): ?array
    {
        $providerFixtures = self::forProvider($provider);

        return $providerFixtures[$key] ?? null;
    }

    /**
     * OpenAI response fixtures.
     */
    protected static function openaiFixtures(): array
    {
        return [
            'hello' => [
                'content' => 'Hello! I\'m ChatGPT, an AI assistant created by OpenAI. How can I help you today?',
                'input_tokens' => 8,
                'output_tokens' => 20,
                'total_tokens' => 28,
                'cost' => 0.00084,
                'input_cost' => 0.00024,
                'output_cost' => 0.0006,
                'finish_reason' => 'stop',
                'model' => 'gpt-4',
                'provider' => 'openai',
            ],
            'code_help' => [
                'content' => "I'd be happy to help you with coding! Here's a simple example:\n\n```php\n<?php\nfunction greet(\$name) {\n    return \"Hello, \" . \$name . \"!\";\n}\n\necho greet(\"World\");\n?>\n```\n\nThis function takes a name parameter and returns a greeting. What specific coding help do you need?",
                'input_tokens' => 15,
                'output_tokens' => 65,
                'total_tokens' => 80,
                'cost' => 0.0024,
                'input_cost' => 0.00045,
                'output_cost' => 0.00195,
                'finish_reason' => 'stop',
                'model' => 'gpt-4',
                'provider' => 'openai',
            ],
            'explain_concept' => [
                'content' => 'Machine learning is a subset of artificial intelligence (AI) that enables computers to learn and improve from experience without being explicitly programmed. Instead of following pre-programmed instructions, ML algorithms build mathematical models based on training data to make predictions or decisions.\n\nKey concepts:\n1. **Training Data**: Examples used to teach the algorithm\n2. **Features**: Input variables used to make predictions\n3. **Model**: The mathematical representation learned from data\n4. **Prediction**: Output generated by the trained model\n\nCommon types include supervised learning (with labeled data), unsupervised learning (finding patterns), and reinforcement learning (learning through rewards).',
                'input_tokens' => 25,
                'output_tokens' => 145,
                'total_tokens' => 170,
                'cost' => 0.00435,
                'input_cost' => 0.00075,
                'output_cost' => 0.0036,
                'finish_reason' => 'stop',
                'model' => 'gpt-4',
                'provider' => 'openai',
            ],
            'function_call' => [
                'content' => null,
                'function_calls' => [
                    [
                        'name' => 'get_weather',
                        'arguments' => ['location' => 'San Francisco, CA'],
                    ],
                ],
                'input_tokens' => 45,
                'output_tokens' => 12,
                'total_tokens' => 57,
                'cost' => 0.00171,
                'input_cost' => 0.00135,
                'output_cost' => 0.00036,
                'finish_reason' => 'function_call',
                'model' => 'gpt-4',
                'provider' => 'openai',
            ],
        ];
    }

    /**
     * xAI (Grok) response fixtures.
     */
    protected static function xaiFixtures(): array
    {
        return [
            'hello' => [
                'content' => 'Hey there! I\'m Grok, your AI assistant with a bit of wit and a lot of knowledge. What can I help you figure out today?',
                'input_tokens' => 8,
                'output_tokens' => 28,
                'total_tokens' => 36,
                'cost' => 0.00108,
                'input_cost' => 0.00024,
                'output_cost' => 0.00084,
                'finish_reason' => 'stop',
                'model' => 'grok-beta',
                'provider' => 'xai',
            ],
            'witty_response' => [
                'content' => 'Well, that\'s a question that would make even a philosopher scratch their head! But let me give it a shot with my usual blend of helpfulness and just a dash of cheekiness.\n\nThe answer, my friend, is both simpler and more complex than you might think. It\'s like asking why pizza tastes better at 2 AM - there are scientific reasons, but sometimes the universe just works in mysterious ways.',
                'input_tokens' => 20,
                'output_tokens' => 75,
                'total_tokens' => 95,
                'cost' => 0.00285,
                'input_cost' => 0.0006,
                'output_cost' => 0.00225,
                'finish_reason' => 'stop',
                'model' => 'grok-beta',
                'provider' => 'xai',
            ],
            'technical_explanation' => [
                'content' => 'Alright, let\'s dive into the technical deep end! *cracks knuckles*\n\nThis is actually a fascinating topic that combines several concepts:\n\n1. **The Core Principle**: Think of it like a digital Swiss Army knife\n2. **Implementation Details**: We\'re dealing with algorithms that would make a mathematician weep tears of joy\n3. **Real-world Applications**: From making your coffee maker smarter to helping rockets find their way to Mars\n\nThe beauty is in how these systems learn and adapt - it\'s like teaching a very eager student who never gets tired and has perfect memory. Pretty cool, right?',
                'input_tokens' => 35,
                'output_tokens' => 125,
                'total_tokens' => 160,
                'cost' => 0.0048,
                'input_cost' => 0.00105,
                'output_cost' => 0.00375,
                'finish_reason' => 'stop',
                'model' => 'grok-beta',
                'provider' => 'xai',
            ],
        ];
    }

    /**
     * Google Gemini response fixtures.
     */
    protected static function geminiFixtures(): array
    {
        return [
            'hello' => [
                'content' => 'Hello! I\'m Gemini, Google\'s AI assistant. I\'m here to help you with information, creative tasks, problem-solving, and more. What would you like to explore today?',
                'input_tokens' => 8,
                'output_tokens' => 32,
                'total_tokens' => 40,
                'cost' => 0.0006,
                'input_cost' => 0.0001,
                'output_cost' => 0.0005,
                'finish_reason' => 'stop',
                'model' => 'gemini-pro',
                'provider' => 'gemini',
            ],
            'multimodal_response' => [
                'content' => 'I can see the image you\'ve shared! This appears to be a beautiful landscape photograph showing mountains in the background with a lake in the foreground. The lighting suggests it was taken during golden hour, creating warm tones across the scene.\n\nKey elements I notice:\n- Mountain range with snow-capped peaks\n- Reflective water surface\n- Natural lighting with warm color palette\n- Composition following rule of thirds\n\nWould you like me to analyze any specific aspects of this image in more detail?',
                'input_tokens' => 1250, // Includes image tokens
                'output_tokens' => 95,
                'total_tokens' => 1345,
                'cost' => 0.01595,
                'input_cost' => 0.01250,
                'output_cost' => 0.00345,
                'finish_reason' => 'stop',
                'model' => 'gemini-pro-vision',
                'provider' => 'gemini',
            ],
            'structured_response' => [
                'content' => '# Analysis Summary\n\n## Key Points\n1. **Primary Factor**: The main driver behind this phenomenon\n2. **Secondary Influences**: Supporting elements that contribute\n3. **Implications**: What this means for practical applications\n\n## Detailed Breakdown\n\n### Technical Aspects\n- **Performance**: Optimized for efficiency\n- **Scalability**: Designed to handle growth\n- **Reliability**: Built with redundancy in mind\n\n### Recommendations\n- Start with a pilot implementation\n- Monitor key metrics closely\n- Iterate based on feedback\n\nThis structured approach ensures comprehensive coverage while maintaining clarity.',
                'input_tokens' => 45,
                'output_tokens' => 135,
                'total_tokens' => 180,
                'cost' => 0.00225,
                'input_cost' => 0.00045,
                'output_cost' => 0.0018,
                'finish_reason' => 'stop',
                'model' => 'gemini-pro',
                'provider' => 'gemini',
            ],
        ];
    }

    /**
     * Ollama response fixtures.
     */
    protected static function ollamaFixtures(): array
    {
        return [
            'hello' => [
                'content' => 'Hello! I\'m running locally through Ollama. I\'m here to help you with various tasks while keeping your data private and secure on your own machine. How can I assist you?',
                'input_tokens' => 8,
                'output_tokens' => 35,
                'total_tokens' => 43,
                'cost' => 0.0, // Local models are free
                'input_cost' => 0.0,
                'output_cost' => 0.0,
                'finish_reason' => 'stop',
                'model' => 'llama2',
                'provider' => 'ollama',
            ],
            'local_advantage' => [
                'content' => 'One of the great advantages of using local AI models like me is privacy and control. Your conversations stay on your machine, you can customize my behavior, and there are no API costs or rate limits to worry about.\n\nI might not be as large as some cloud-based models, but I\'m:\n- Always available offline\n- Completely private\n- Customizable for your specific needs\n- Free to use as much as you want\n\nWhat would you like to explore together?',
                'input_tokens' => 25,
                'output_tokens' => 85,
                'total_tokens' => 110,
                'cost' => 0.0,
                'input_cost' => 0.0,
                'output_cost' => 0.0,
                'finish_reason' => 'stop',
                'model' => 'llama2',
                'provider' => 'ollama',
            ],
        ];
    }

    /**
     * Generic response fixtures.
     */
    protected static function genericFixtures(): array
    {
        return [
            'default' => [
                'content' => 'This is a mock response from the AI provider. It demonstrates the basic functionality of the system.',
                'input_tokens' => 10,
                'output_tokens' => 20,
                'total_tokens' => 30,
                'cost' => 0.0015,
                'input_cost' => 0.0005,
                'output_cost' => 0.001,
                'finish_reason' => 'stop',
                'model' => 'mock-model',
                'provider' => 'mock',
            ],
            'short' => [
                'content' => 'Yes.',
                'input_tokens' => 5,
                'output_tokens' => 1,
                'total_tokens' => 6,
                'cost' => 0.0003,
                'input_cost' => 0.0002,
                'output_cost' => 0.0001,
                'finish_reason' => 'stop',
                'model' => 'mock-model',
                'provider' => 'mock',
            ],
            'long' => [
                'content' => 'This is a much longer response that demonstrates how the system handles extended content. It includes multiple sentences and provides detailed information about the topic at hand. The response continues with additional context and examples to thoroughly address the user\'s query. This type of response is useful for testing token counting, cost calculation, and streaming functionality. The content is designed to be realistic while remaining generic enough to be useful across different testing scenarios.',
                'input_tokens' => 15,
                'output_tokens' => 85,
                'total_tokens' => 100,
                'cost' => 0.005,
                'input_cost' => 0.0015,
                'output_cost' => 0.0035,
                'finish_reason' => 'stop',
                'model' => 'mock-model',
                'provider' => 'mock',
            ],
        ];
    }

    /**
     * Error response fixtures.
     */
    protected static function errorFixtures(): array
    {
        return [
            'rate_limit' => [
                'error' => 'rate_limit',
                'message' => 'Rate limit exceeded. Please try again later.',
                'retry_after' => 60,
                'provider' => 'mock',
            ],
            'timeout' => [
                'error' => 'timeout',
                'message' => 'Request timeout. The server took too long to respond.',
                'provider' => 'mock',
            ],
            'invalid_credentials' => [
                'error' => 'invalid_credentials',
                'message' => 'Invalid API key or credentials.',
                'provider' => 'mock',
            ],
            'model_not_found' => [
                'error' => 'model_not_found',
                'message' => 'The specified model was not found.',
                'model' => 'nonexistent-model',
                'provider' => 'mock',
            ],
            'insufficient_quota' => [
                'error' => 'insufficient_quota',
                'message' => 'Insufficient quota. Please check your billing.',
                'provider' => 'mock',
            ],
            'content_filter' => [
                'error' => 'content_filter',
                'message' => 'Content was filtered due to policy violations.',
                'provider' => 'mock',
            ],
        ];
    }
}
